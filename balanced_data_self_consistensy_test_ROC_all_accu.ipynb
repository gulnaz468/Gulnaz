import csv
import pickle
import seaborn as sns
from sklearn import decomposition
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import matthews_corrcoef
sns.set(color_codes=True)
%matplotlib inline 
# Import necessary modules
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import LeavePOut
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import StratifiedKFold
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# roc curve and auc score
from sklearn.datasets import make_classification
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
#data_frame = pd.read_csv("/content/drive/MyDrive/hemolysis/FVs-Merged-Labled-pos_neg.csv")
data_frame = pd.read_csv("/content/drive/MyDrive/hemolysis/FVs-labled-balanced_data.csv")
print (data_frame.isnull().values.any())
def plot_corr(data_frame, size=11):
    """
    Function plots a graphical correlation matrix for each pair of columns in the dataframe.

    Input:
        data_frame: pandas DataFrame
        size: vertical and horizontal size of the plot

    Displays:
        matrix of correlation between columns.  Blue-cyan-yellow-red-darkred => less to more correlated
                                                0 ------------------>  1
                                                Expect a darkred line running from top left to bottom right
    """

    corr = data_frame.corr()    # data frame correlation function
    fig, ax = plt.subplots(figsize=(size, size))
    ax.matshow(corr)   # color code the rectangles by correlation value
    plt.xticks(range(len(corr.columns)), corr.columns)  # draw x tick marks
    plt.yticks(range(len(corr.columns)), corr.columns)  # draw y tick marks

num_obs = len(data_frame)
num_true = len(data_frame.loc[data_frame['Positive'] == 1])
print(num_true)

num_false = len(data_frame.loc[data_frame['Nagetive'] == 1])

print(num_false)

print("Number of True cases:  {0} ({1:2.2f}%)".format(num_true, ((1.00 * num_true)/(1.0 * num_obs)) * 100))
print("Number of False cases: {0} ({1:2.2f}%)".format(num_false, (( 1.0 * num_false)/(1.0 * num_obs)) * 100))

#from sklearn.cross_validation import train_test_split

#feature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']
predicted_class_names = ['Positive','Nagetive']

# Separating out the target
y = data_frame.loc[:,['Positive']].values

df2 = data_frame.drop(['Positive','Nagetive'], axis=1)
df2.reset_index(inplace=True)
# Separating out the features
x = df2.values
#print(x)

# Standardizing the features
x = StandardScaler().fit_transform(x)
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# generate two class dataset
#X, y = make_classification(n_samples=1000, n_classes=2, n_features=20, random_state=27)

# split into train-test sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=27)
# train models
import sklearn.metrics as metrics
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
# XGBClassifier
model1 = XGBClassifier(random_state=42)
# knn
model2 = KNeighborsClassifier(n_neighbors=4)
#SVC
model3 = SVC(kernel='linear', C=1, random_state=42,probability=True)
#RandomForestClassifier
model4 = RandomForestClassifier(random_state=42)
#AdaBoostClassifier
model5 = AdaBoostClassifier(random_state=42)
#DecisionTreeClassifier
model6 = DecisionTreeClassifier(random_state=42)
#MLPClassifier
model7 =MLPClassifier (hidden_layer_sizes=(13,13,13),max_iter=500, random_state=42)






# fit model
model1.fit(X_train, y_train)
model2.fit(X_train, y_train)
model3.fit(X_train, y_train)
model4.fit(X_train, y_train)
model5.fit(X_train, y_train)
model6.fit(X_train, y_train)
model7.fit(X_train, y_train)

training_start = time.perf_counter()
xgb_predict_test = model1.predict(X_train)
#get accuracy
xgb_accuracy_testdata = metrics.accuracy_score(y_train, xgb_predict_test)
#print accuracy
print ("Accuracy of xgboost: {0:.4f}".format(xgb_accuracy_testdata))
training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing 1XGB in independent test: %4.3f" % (model_train_time))


training_start = time.perf_counter()
knn_predict_test = model2.predict(X_train)
#get accuracy
knn_accuracy_testdata = metrics.accuracy_score(y_train, knn_predict_test)
#print accuracy
print ("Accuracy of KNN: {0:.4f}".format(knn_accuracy_testdata))
training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing 2_KNN in independent test: %4.3f" % (model_train_time))

training_start = time.perf_counter()
svm_predict_test = model3.predict(X_train)
#get accuracy
svm_accuracy_testdata = metrics.accuracy_score(y_train, svm_predict_test)
#print accuracy
print ("Accuracy of svm: {0:.4f}".format(svm_accuracy_testdata))
training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing SVM_3 in independent test: %4.3f" % (model_train_time))

training_start = time.perf_counter()
rf_predict_test = model4.predict(X_train)
#get accuracy
rf_accuracy_testdata = metrics.accuracy_score(y_train, rf_predict_test)
#print accuracy
print ("Accuracy of rendom forest: {0:.4f}".format(rf_accuracy_testdata))
training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing RF_4 in independent test: %4.3f" % (model_train_time))


training_start = time.perf_counter()
adabost_predict_test = model5.predict(X_train)
#get accuracy
adabost_accuracy_testdata = metrics.accuracy_score(y_train, adabost_predict_test)
#print accuracy
print ("Accuracy of Adabooost: {0:.4f}".format(adabost_accuracy_testdata))
training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing Adaboost_5 in independent test: %4.3f" % (model_train_time))


training_start = time.perf_counter()
dt_predict_test = model6.predict(X_train)
#get accuracy
dt_accuracy_testdata = metrics.accuracy_score(y_train, dt_predict_test)
#print accuracy
print ("Accuracy of DT: {0:.4f}".format(dt_accuracy_testdata))
training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing DT_6 in independent test: %4.3f" % (model_train_time))



training_start = time.perf_counter()
nn_predict_test = model7.predict(X_train)
#get accuracy
nn_accuracy_testdata = metrics.accuracy_score(y_train, nn_predict_test)
#print accuracy
print ("Accuracy of NN: {0:.4f}".format(nn_accuracy_testdata))
training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing NN_7 in independent test: %4.3f" % (model_train_time))


# predict probabilities
pred_prob1 = model1.predict_proba(X_train)
pred_prob2 = model2.predict_proba(X_train)
pred_prob3 = model3.predict_proba(X_train)
pred_prob4 = model4.predict_proba(X_train)
pred_prob5 = model5.predict_proba(X_train)
pred_prob6 = model6.predict_proba(X_train)
pred_prob7 = model7.predict_proba(X_train)

#for model XGBOOST
cm = confusion_matrix(y_train, knn_predict_test, labels=[1, 0]).ravel()
print(cm)
tp, fp, fn, tn = confusion_matrix(y_train, knn_predict_test, labels=[1, 0]).ravel()
np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_train, xgb_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(X_train)
pickle.dump(model1, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in XGBOOST-Results_selfconsistancyt test.csv')
with open('./Results are Saved in XGBOOST-Results_selfconsistancyt test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['independent test for xgboost'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])
#for model KNN
cm = confusion_matrix(y_train, knn_predict_test, labels=[1, 0]).ravel()
print(cm)
tp, fp, fn, tn = confusion_matrix(y_train, knn_predict_test, labels=[1, 0]).ravel()
np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_train, knn_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(X_train)
pickle.dump(model2, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in knn_selfconsistansy.csv')
with open('./Results are Saved in knn_selfconsistansy.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['independent test for KNN'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])
#for model NN
cm = confusion_matrix(y_train, svm_predict_test, labels=[1, 0]).ravel()
print(cm)
tp, fp, fn, tn = confusion_matrix(y_train, svm_predict_test, labels=[1, 0]).ravel()
np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_train, svm_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(X_train)
pickle.dump(model3, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in SVM_consistansy.csv')
with open('./SVM_consistansy.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['SVM_consistansy test for SVM'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])
#for model RF
cm = confusion_matrix(y_train, rf_predict_test, labels=[1, 0]).ravel()
print(cm)
tp, fp, fn, tn = confusion_matrix(y_train, rf_predict_test, labels=[1, 0]).ravel()
np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_train, rf_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(X_train)
pickle.dump(model4, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in RF_selfconsistansy.csv')
with open('./RF_selfconsistansy.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['RF_selfconsistansy test for RF'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])
#for model adabost
cm = confusion_matrix(y_train, adabost_predict_test, labels=[1, 0]).ravel()
print(cm)
tp, fp, fn, tn = confusion_matrix(y_train, adabost_predict_test, labels=[1, 0]).ravel()
np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_train, adabost_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(X_train)
pickle.dump(model6, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved adabost_selfconsistansyt.csv')
with open('./adabost_selfconsistansyt.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['adabost_selfconsistansyt test for adaboost'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])
#for model DT
cm = confusion_matrix(y_train, dt_predict_test, labels=[1, 0]).ravel()
print(cm)
tp, fp, fn, tn = confusion_matrix(y_train, dt_predict_test, labels=[1, 0]).ravel()
np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_train, dt_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(X_train)
pickle.dump(model6, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved DT_selfconsistancy.csv')
with open('./DT_selfconsistancy.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['DT_selfconsistancy test for DT'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])
#for model NN
cm = confusion_matrix(y_train, nn_predict_test, labels=[1, 0]).ravel()
print(cm)
tp, fp, fn, tn = confusion_matrix(y_train, nn_predict_test, labels=[1, 0]).ravel()
np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_train, nn_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(X_train)
pickle.dump(model7, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in NN_selfconsistansy.csv')
with open('./NN_selfconsistansy test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['NN_selfconsistansy test for NN'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])
from sklearn.metrics import roc_curve

# roc curve for models
fpr1, tpr1, thresh1 = roc_curve(y_train, pred_prob1[:,1], pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_train, pred_prob2[:,1], pos_label=1)
fpr3, tpr3, thresh3 = roc_curve(y_train, pred_prob3[:,1], pos_label=1)
fpr4, tpr4, thresh4 = roc_curve(y_train, pred_prob4[:,1], pos_label=1)
fpr5, tpr5, thresh5 = roc_curve(y_train, pred_prob5[:,1], pos_label=1)
fpr6, tpr6, thresh6 = roc_curve(y_train, pred_prob6[:,1], pos_label=1)
fpr7, tpr7, thresh7 = roc_curve(y_train, pred_prob7[:,1], pos_label=1)
# roc curve for tpr = fpr 
i=6,
random_probs = [0 for i in range(len(y_train))]
p_fpr, p_tpr, _ = roc_curve(y_train, random_probs, pos_label=1)
from sklearn.metrics import roc_auc_score

# auc scores
auc_score1 = roc_auc_score(y_train, pred_prob1[:,1])
auc_score2 = roc_auc_score(y_train, pred_prob2[:,1])
auc_score3 = roc_auc_score(y_train, pred_prob3[:,1])
auc_score4 = roc_auc_score(y_train, pred_prob4[:,1])
auc_score5 = roc_auc_score(y_train, pred_prob5[:,1])
auc_score6 = roc_auc_score(y_train, pred_prob6[:,1])
auc_score7 = roc_auc_score(y_train, pred_prob7[:,1])

print(auc_score1, auc_score2,auc_score3,auc_score4,auc_score5,auc_score6,auc_score7)
# matplotlib
import matplotlib.pyplot as plt
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr1, tpr1, linestyle='solid',color='silver', label='XGB')
plt.plot(fpr2, tpr2, linestyle='solid',color='sandybrown', label='KNN')
plt.plot(fpr3, tpr3, linestyle='solid',color='yellow', label='SVC')
plt.plot(fpr4, tpr4, linestyle='solid',color='cyan', label='RandomForest')
plt.plot(fpr5, tpr5, linestyle='solid',color='magenta', label='AdaBoost')
plt.plot(fpr6, tpr6, linestyle='solid',color='black', label='DecisionTree')
plt.plot(fpr7, tpr7, linestyle='solid',color='lightgreen', label='NN')
plt.plot(p_fpr, p_tpr, linestyle='solid', color='blue')
# title
plt.title('ROC curve Self consistancy test')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC curve Self consistancy test',dpi=300)
plt.show();
