import csv
import pickle
import seaborn as sns
from sklearn import decomposition
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import matthews_corrcoef
sns.set(color_codes=True)
%matplotlib inline 
# Import necessary modules
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import LeavePOut
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import StratifiedKFold
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# roc curve and auc score
from sklearn.datasets import make_classification
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

data_frame = pd.read_csv("/content/drive/MyDrive/hemolysis/FVs-labled-balanced_data.csv")

print (data_frame.isnull().values.any())

def plot_corr(data_frame, size=11):
    """
    Function plots a graphical correlation matrix for each pair of columns in the dataframe.

    Input:
        data_frame: pandas DataFrame
        size: vertical and horizontal size of the plot

    Displays:
        matrix of correlation between columns.  Blue-cyan-yellow-red-darkred => less to more correlated
                                                0 ------------------>  1
                                                Expect a darkred line running from top left to bottom right
    """

    corr = data_frame.corr()    # data frame correlation function
    fig, ax = plt.subplots(figsize=(size, size))
    ax.matshow(corr)   # color code the rectangles by correlation value
    plt.xticks(range(len(corr.columns)), corr.columns)  # draw x tick marks
    plt.yticks(range(len(corr.columns)), corr.columns)  # draw y tick marks


plot_corr(data_frame)

num_obs = len(data_frame)
num_true = len(data_frame.loc[data_frame['Positive'] == 1])
print(num_true)

num_false = len(data_frame.loc[data_frame['Nagetive'] == 1])

print(num_false)

print("Number of True cases:  {0} ({1:2.2f}%)".format(num_true, ((1.00 * num_true)/(1.0 * num_obs)) * 100))
print("Number of False cases: {0} ({1:2.2f}%)".format(num_false, (( 1.0 * num_false)/(1.0 * num_obs)) * 100))


#from sklearn.cross_validation import train_test_split

#feature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']
predicted_class_names = ['Positive','Nagetive']

# Separating out the target
y = data_frame.loc[:,['Positive']].values

df2 = data_frame.drop(['Positive','Nagetive'], axis=1)
df2.reset_index(inplace=True)
# Separating out the features
x = df2.values
#print(x)

# Standardizing the features
x = StandardScaler().fit_transform(x)


# evaluate a logistic regression model using k-fold cross-validation
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from numpy import mean
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate

# train models
import sklearn.metrics as metrics
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
loocv = model_selection.LeaveOneOut()


from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score

scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}



# XGBClassifier
model1 = XGBClassifier(random_state=42)
# knn
model2 = KNeighborsClassifier(n_neighbors=4)
#SVC
model3 = SVC(kernel='linear', C=1, random_state=42,probability=True)
#RandomForestClassifier
model4 = RandomForestClassifier(random_state=42)
#AdaBoostClassifier
model5 = AdaBoostClassifier(random_state=42)
#DecisionTreeClassifier
model6 = DecisionTreeClassifier(random_state=42)
#MLPClassifier
model7 =MLPClassifier (hidden_layer_sizes=(13,13,13),max_iter=500, random_state=42)

training_start = time.perf_counter()
 #for model 1 XGB

training_start = time.perf_counter()
#cv = KFold(random_state=42)
# evaluate model

scores1 = cross_validate(model1, x, y, scoring=scoring, cv=loocv, n_jobs=-1, return_train_score=True)
print(scores1.keys())

print(scores1['test_accuracy']) 
print('Accuracy: %.3f (%.3f)' % (mean(scores1['test_accuracy']), std(scores1['test_accuracy'])))

print(scores1['fit_time'])
print('fit_time: %.3f (%.3f)' % (mean(scores1['fit_time']), std(scores1['fit_time'])))


print(scores1['test_precision'])
print('test_precision: %.3f (%.3f)' % (mean(scores1['test_precision']), std(scores1['test_precision'])))


print(scores1['train_precision'])
print('train_precision: %.3f (%.3f)' % (mean(scores1['train_precision']), std(scores1['train_precision'])))


print(scores1['test_f1_score'])
print('test_f1_score: %.3f (%.3f)' % (mean(scores1['test_f1_score']), std(scores1['test_f1_score'])))

print(scores1['train_f1_score'])
print('train_f1_score: %.3f (%.3f)' % (mean(scores1['train_f1_score']), std(scores1['train_f1_score'])))

print(scores1['test_recall'])
print('test_recall: %.3f (%.3f)' % (mean(scores1['test_recall']), std(scores1['test_recall'])))

print(scores1['train_recall'])
print('train_recall: %.3f (%.3f)' % (mean(scores1['train_recall']), std(scores1['train_recall'])))
training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing xgboost in independent test: %4.3f" % (model_train_time))


#for model 2 KNN

training_start = time.perf_counter()
#cv = KFold(n_splits=10, random_state=42, shuffle=True)
# evaluate model

scores2 = cross_validate(model2, x, y, scoring=scoring, cv=loocv, n_jobs=-1, return_train_score=True)
print(scores2.keys())

print(scores2['test_accuracy'])
print('Accuracy: %.3f (%.3f)' % (mean(scores2['test_accuracy']), std(scores2['test_accuracy'])))

print(scores2['fit_time'])
print('fit_time: %.3f (%.3f)' % (mean(scores2['fit_time']), std(scores2['fit_time'])))

print(scores2['test_recall'])
print('test_recall: %.3f (%.3f)' % (mean(scores2['test_recall']), std(scores2['test_recall'])))


print(scores2['test_precision'])
print('test_precision: %.3f (%.3f)' % (mean(scores2['test_precision']), std(scores2['test_precision'])))


print(scores2['test_f1_score'])
print('test_f1_score: %.3f (%.3f)' % (mean(scores2['test_f1_score']), std(scores2['test_f1_score'])))


training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing KNN in independent test: %4.3f" % (model_train_time))

#for model 3 SVM

training_start = time.perf_counter()
#cv = KFold(n_splits=10, random_state=42, shuffle=True)
# evaluate model

scores3 = cross_validate(model3, x, y, scoring=scoring, cv=loocv, n_jobs=-1, return_train_score=True)
print(scores3.keys())

print(scores3['test_accuracy']) 
print('Accuracy: %.3f (%.3f)' % (mean(scores3['test_accuracy']), std(scores3['test_accuracy'])))

print(scores3['fit_time'])
print('fit_time: %.3f (%.3f)' % (mean(scores3['fit_time']), std(scores3['fit_time'])))

print(scores3['test_recall'])
print('test_recall: %.3f (%.3f)' % (mean(scores3['test_recall']), std(scores3['test_recall'])))


print(scores3['test_precision'])
print('test_precision: %.3f (%.3f)' % (mean(scores3['test_precision']), std(scores3['test_precision'])))


print(scores3['test_f1_score'])
print('test_f1_score: %.3f (%.3f)' % (mean(scores3['test_f1_score']), std(scores3['test_f1_score'])))

training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing SVM in independent test: %4.3f" % (model_train_time))

training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing SVM in independent test: %4.3f" % (model_train_time))


#for model 4=RF

training_start = time.perf_counter()
#cv = KFold(n_splits=10, random_state=42, shuffle=True)
# evaluate model

scores4 = cross_validate(model4, x, y, scoring=scoring, cv=loocv, n_jobs=-1, return_train_score=True)
print(scores4.keys())

print(scores4['test_accuracy']) 
print('Accuracy: %.3f (%.3f)' % (mean(scores4['test_accuracy']), std(scores4['test_accuracy'])))

print(scores4['fit_time'])
print('fit_time: %.3f (%.3f)' % (mean(scores4['fit_time']), std(scores4['fit_time'])))

print(scores4['test_recall'])
print('test_recall: %.3f (%.3f)' % (mean(scores4['test_recall']), std(scores4['test_recall'])))


print(scores4['test_precision'])
print('test_precision: %.3f (%.3f)' % (mean(scores4['test_precision']), std(scores4['test_precision'])))


print(scores4['test_f1_score'])
print('test_f1_score: %.3f (%.3f)' % (mean(scores4['test_f1_score']), std(scores4['test_f1_score'])))

training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing RF in independent test: %4.3f" % (model_train_time))


#for model 5= Adaboost

training_start = time.perf_counter()
#cv = KFold(n_splits=10, random_state=42, shuffle=True)
# evaluate model

scores5 = cross_validate(model5, x, y, scoring=scoring, cv=loocv, n_jobs=-1, return_train_score=True)
print(scores5.keys())

print(scores5['test_accuracy']) 
print('Accuracy: %.3f (%.3f)' % (mean(scores5['test_accuracy']), std(scores5['test_accuracy'])))

print(scores5['fit_time'])
print('fit_time: %.3f (%.3f)' % (mean(scores5['fit_time']), std(scores5['fit_time'])))

print(scores5['test_recall'])
print('test_recall: %.3f (%.3f)' % (mean(scores5['test_recall']), std(scores5['test_recall'])))


print(scores5['test_precision'])
print('test_precision: %.3f (%.3f)' % (mean(scores5['test_precision']), std(scores5['test_precision'])))


print(scores5['test_f1_score'])
print('test_f1_score: %.3f (%.3f)' % (mean(scores5['test_f1_score']), std(scores5['test_f1_score'])))

training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing adaboost in independent test: %4.3f" % (model_train_time))


#for model 6= DT

training_start = time.perf_counter()
#cv = KFold(n_splits=10, random_state=42, shuffle=True)
# evaluate model

scores6 = cross_validate(model6, x, y, scoring=scoring, cv=loocv, n_jobs=-1, return_train_score=True)
print(scores6.keys())

print(scores6['test_accuracy']) 
print('Accuracy: %.3f (%.3f)' % (mean(scores6['test_accuracy']), std(scores6['test_accuracy'])))

print(scores6['fit_time'])
print('fit_time: %.3f (%.3f)' % (mean(scores6['fit_time']), std(scores6['fit_time'])))

print(scores6['test_recall'])
print('test_recall: %.3f (%.3f)' % (mean(scores6['test_recall']), std(scores6['test_recall'])))


print(scores6['test_precision'])
print('test_precision: %.3f (%.3f)' % (mean(scores6['test_precision']), std(scores6['test_precision'])))


print(scores6['test_f1_score'])
print('test_f1_score: %.3f (%.3f)' % (mean(scores6['test_f1_score']), std(scores6['test_f1_score'])))

training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing DT in independent test: %4.3f" % (model_train_time))


#for model 7= NN

training_start = time.perf_counter()
#cv = KFold(n_splits=10, random_state=42, shuffle=True)
# evaluate model

scores7 = cross_validate(model7, x, y, scoring=scoring, cv=loocv, n_jobs=-1, return_train_score=True)
print(scores7.keys())

print(scores7['test_accuracy']) 
print('Accuracy: %.3f (%.3f)' % (mean(scores7['test_accuracy']), std(scores7['test_accuracy'])))

print(scores7['fit_time'])
print('fit_time: %.3f (%.3f)' % (mean(scores7['fit_time']), std(scores7['fit_time'])))

print(scores7['test_recall'])
print('test_recall: %.3f (%.3f)' % (mean(scores7['test_recall']), std(scores7['test_recall'])))


print(scores7['test_precision'])
print('test_precision: %.3f (%.3f)' % (mean(scores7['test_precision']), std(scores7['test_precision'])))


print(scores7['test_f1_score'])
print('test_f1_score: %.3f (%.3f)' % (mean(scores7['test_f1_score']), std(scores7['test_f1_score'])))

training_end = time.perf_counter()
model_train_time = training_end-training_start
print("Time consumed for testing NN in independent test: %4.3f" % (model_train_time))


#for model XGBOOST
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
xgb_predict_test = cross_val_predict(model1, x, y, cv=loocv)
y_test=y
tp, fp, fn, tn = confusion_matrix(y, xgb_predict_test, labels=[1, 0]).ravel()

np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_test, xgb_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(x)
pickle.dump(model1, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in XGBOOST-crossvalidation test.csv')
with open('./Results are Saved in XGBOOST-crossvalidation test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['crossvalidation for xgboost'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])

#for model KNN
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
knn_predict_test = cross_val_predict(model2, x, y, cv=loocv)
y_test=y
tp, fp, fn, tn = confusion_matrix(y, knn_predict_test, labels=[1, 0]).ravel()

np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_test, knn_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(x)
pickle.dump(model2, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in knn-crossvalidation test.csv')
with open('./Results are Saved in knn-crossvalidation test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['crossvalidation for knn'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])

#for model SVM=3
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
svm_predict_test = cross_val_predict(model3, x, y, cv=loocv)
y_test=y
tp, fp, fn, tn = confusion_matrix(y, svm_predict_test, labels=[1, 0]).ravel()

np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_test, svm_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(x)
pickle.dump(model3, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in svm-crossvalidation test.csv')
with open('./Results are Saved in svm-crossvalidation test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['crossvalidation for svm'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])

#for model RF=4
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
rf_predict_test = cross_val_predict(model4, x, y, cv=loocv)
y_test=y
tp, fp, fn, tn = confusion_matrix(y, rf_predict_test, labels=[1, 0]).ravel()

np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_test, rf_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(x)
pickle.dump(model4, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in RF-crossvalidation test.csv')
with open('./Results are Saved in RF-crossvalidation test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['crossvalidation for RF'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])

#for model adabost=5
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
adabost_predict_test = cross_val_predict(model5, x, y, cv=loocv)
y_test=y
tp, fp, fn, tn = confusion_matrix(y, adabost_predict_test, labels=[1, 0]).ravel()

np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_test, adabost_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(x)
pickle.dump(model5, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in adabost-crossvalidation test.csv')
with open('./Results are Saved in adabost-crossvalidation test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['crossvalidation for adabost'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])




#for model DT=6
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
dt_predict_test = cross_val_predict(model6, x, y, cv=loocv)
y_test=y
tp, fp, fn, tn = confusion_matrix(y, dt_predict_test, labels=[1, 0]).ravel()

np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_test, dt_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(x)
pickle.dump(model6, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in DT-crossvalidation test.csv')
with open('./Results are Saved in DT-crossvalidation test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['crossvalidation for DT'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])

#for model NN7
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
nn_predict_test = cross_val_predict(model7, x, y, cv=loocv)
y_test=y
tp, fp, fn, tn = confusion_matrix(y, nn_predict_test, labels=[1, 0]).ravel()

np.random.seed(7)
acc = np.round(((tn + tp) / (tn + fp + fn + tp)) * 100, 2)
sp = np.round((tn / (fp + tn)) * 100, 2)
sn = np.round((tp / (tp + fn)) * 100, 2)
mcc = np.round(matthews_corrcoef(y_test, nn_predict_test), 5)
print([tp, fp, tn, fn, acc, sp, sn, mcc])
self_scores=[]
self_scores.append([tp, fp, tn, fn, acc, sp, sn, mcc])
std_scale = StandardScaler().fit(x)
pickle.dump(model7, open('./iphosd_Model.pkl', 'wb'))
pickle.dump(std_scale, open('./iphosd_Scale.pkl', 'wb'))

print('\n\nResults are Saved in NN-crossvalidation test.csv')
with open('./Results are Saved in NN-crossvalidation test.csv', 'w', newline='') as csvfile:
    resultwriter = csv.writer(csvfile, delimiter=',', quotechar='|')
    resultwriter.writerow(['crossvalidation for NN'])
    resultwriter.writerow(
        ['True Positive', 'False Positive', 'True Negative', 'False Negative', 'Accuracy', 'Specificity', 'Sensitivity',
         'MCC'])
    resultwriter.writerow(self_scores[0])
# fit model
model1.fit( x, y.ravel())
model2.fit( x, y.ravel())
model3.fit( x, y.ravel())
model4.fit( x, y.ravel())
model5.fit( x, y.ravel())
model6.fit( x, y.ravel())
model7.fit( x, y.ravel())


from sklearn.metrics import roc_curve
from sklearn.model_selection import cross_val_predict
pred_prob1 = cross_val_predict(model1, x, y.ravel(), cv=cv, method='predict_proba')
fpr1, tpr1, thresh1 = roc_curve(y.ravel(), pred_prob1[:,1], pos_label=1)

pred_prob2 = cross_val_predict(model2, x, y.ravel(), cv=cv, method='predict_proba')
fpr2, tpr2, thresh2 = roc_curve(y.ravel(), pred_prob2[:,1], pos_label=1)

pred_prob3 = cross_val_predict(model3, x, y.ravel(), cv=cv, method='predict_proba')
fpr3, tpr3, thresh3 = roc_curve(y.ravel(), pred_prob3[:,1], pos_label=1)

pred_prob4 = cross_val_predict(model4, x, y.ravel(), cv=cv, method='predict_proba')
fpr4, tpr4, thresh4 = roc_curve(y.ravel(), pred_prob4[:,1], pos_label=1)

pred_prob5 = cross_val_predict(model5, x, y.ravel(), cv=cv, method='predict_proba')
fpr5, tpr5, thresh5 = roc_curve(y.ravel(), pred_prob5[:,1], pos_label=1)

pred_prob6 = cross_val_predict(model6, x, y.ravel(), cv=cv, method='predict_proba')
fpr6, tpr6, thresh6 = roc_curve(y.ravel(), pred_prob6[:,1], pos_label=1)

pred_prob7 = cross_val_predict(model7, x, y.ravel(), cv=cv, method='predict_proba')
fpr7, tpr7, thresh7 = roc_curve(y.ravel(), pred_prob7[:,1], pos_label=1)


from sklearn.metrics import roc_auc_score

# auc scores
auc_score1 = roc_auc_score(y.ravel(), pred_prob1[:,1])
auc_score2 = roc_auc_score(y.ravel(), pred_prob2[:,1])
auc_score3 = roc_auc_score(y.ravel(), pred_prob3[:,1])
auc_score4 = roc_auc_score(y.ravel(), pred_prob4[:,1])
auc_score5 = roc_auc_score(y.ravel(), pred_prob5[:,1])
auc_score6 = roc_auc_score(y.ravel(), pred_prob6[:,1])
auc_score7 = roc_auc_score(y.ravel(), pred_prob7[:,1])

print(auc_score1, auc_score2,auc_score3,auc_score4,auc_score5,auc_score6,auc_score7)
i=6,
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)


# matplotlib
import matplotlib.pyplot as plt
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr1, tpr1, linestyle='solid',color='silver', label='XGB')
plt.plot(fpr2, tpr2, linestyle='solid',color='sandybrown', label='KNN')
plt.plot(fpr3, tpr3, linestyle='solid',color='yellow', label='SVM')
plt.plot(fpr4, tpr4, linestyle='solid',color='cyan', label='RandomForest')
plt.plot(fpr5, tpr5, linestyle='solid',color='magenta', label='AdaBoost')
plt.plot(fpr6, tpr6, linestyle='solid',color='black', label='DecisionTree')
plt.plot(fpr7, tpr7, linestyle='solid',color='lightgreen', label='NN')
plt.plot(p_fpr, p_tpr, linestyle='solid', color='blue')
# title
plt.title('ROC curve Jack knife test')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC curve Jack knife test',dpi=300)
plt.show();
